{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kepek_mabur</td>\n",
       "      <td>Ini baguss sihh keren pemandangan nya ya gak s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>primahermanto</td>\n",
       "      <td>@vebeeandyany Ayoooo gasss..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happshapist</td>\n",
       "      <td>@kikideva_ Monggo sek kemarin tanya2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yuliana_iin</td>\n",
       "      <td>kii lhoo @siscaherning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afiq5922</td>\n",
       "      <td>@umami_resa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username                                            comment\n",
       "0    kepek_mabur  Ini baguss sihh keren pemandangan nya ya gak s...\n",
       "1  primahermanto                       @vebeeandyany Ayoooo gasss..\n",
       "2    happshapist               @kikideva_ Monggo sek kemarin tanya2\n",
       "3    yuliana_iin                             kii lhoo @siscaherning\n",
       "4       afiq5922                                        @umami_resa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "LOAD_DATA = pd.read_csv(\"comments.csv\", encoding = \"ISO-8859-1\")\n",
    "LOAD_DATA['comment'].str.encode('ascii', 'ignore')\n",
    "LOAD_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0    ini baguss sihh keren pemandangan nya ya gak s...\n",
      "1                         @vebeeandyany ayoooo gasss..\n",
      "2                 @kikideva_ monggo sek kemarin tanya2\n",
      "3                               kii lhoo @siscaherning\n",
      "4                                          @umami_resa\n",
      "Name: comment, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(LOAD_DATA['comment'].head(5))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [ini, baguss, sihh, keren, pemandangan, nya, y...\n",
      "1                                      [ayoooo, gasss]\n",
      "2                        [monggo, sek, kemarin, tanya]\n",
      "3                                          [kii, lhoo]\n",
      "4                                               [resa]\n",
      "Name: comment_tokens, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "LOAD_DATA['comment'] = LOAD_DATA['comment'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "LOAD_DATA['comment_tokens'] = LOAD_DATA['comment'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(LOAD_DATA['comment_tokens'].head())\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0    [(ini, 1), (baguss, 1), (sihh, 1), (keren, 1),...\n",
      "1                            [(ayoooo, 1), (gasss, 1)]\n",
      "2    [(monggo, 1), (sek, 1), (kemarin, 1), (tanya, 1)]\n",
      "3                                [(kii, 1), (lhoo, 1)]\n",
      "4                                          [(resa, 1)]\n",
      "Name: comment_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "LOAD_DATA['comment_tokens_fdist'] = LOAD_DATA['comment_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(LOAD_DATA['comment_tokens_fdist'].head().apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n",
      "0    [baguss, sihh, keren, pemandangan]\n",
      "1                       [ayoooo, gasss]\n",
      "2                     [monggo, kemarin]\n",
      "3                           [kii, lhoo]\n",
      "4                                [resa]\n",
      "Name: comment_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "print(len(list_stopwords))\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "len(list_stopwords)\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "len(list_stopwords)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "LOAD_DATA['comment_tokens_WSW'] = LOAD_DATA['comment_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(LOAD_DATA['comment_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [bagus, sihh, keren, pemandangan]\n",
       "1                      [ayoooo, gasss]\n",
       "2                    [monggo, kemarin]\n",
       "3                          [kii, lhoo]\n",
       "4                               [resa]\n",
       "5                       [nama, tempat]\n",
       "6                               [ehem]\n",
       "7                               [zain]\n",
       "8            [adminnya, kesana, belom]\n",
       "9                                   []\n",
       "Name: comment_normalized, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizad_word = pd.read_excel(\"Normalisasi.xlsx\")\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "LOAD_DATA['comment_normalized'] = LOAD_DATA['comment_tokens_WSW'].apply(normalized_term)\n",
    "\n",
    "LOAD_DATA['comment_normalized'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "------------------------\n",
      "bagus : bagus\n",
      "sihh : sihh\n",
      "keren : keren\n",
      "pemandangan : pandang\n",
      "ayoooo : ayoooo\n",
      "gasss : gasss\n",
      "monggo : monggo\n",
      "kemarin : kemarin\n",
      "kii : kii\n",
      "lhoo : lhoo\n",
      "resa : resa\n",
      "nama : nama\n",
      "tempat : tempat\n",
      "ehem : ehem\n",
      "zain : zain\n",
      "adminnya : adminnya\n",
      "kesana : kesana\n",
      "belom : bom\n",
      "merapat : rapat\n",
      "makasih : makasih\n",
      "info : info\n",
      "nongkrong : nongkrong\n",
      "tanjung : tanjung\n",
      "harjo : harjo\n",
      "syahdu : syahdu\n",
      "poll : poll\n",
      "donggg : donggg\n",
      "gaa : gaa\n",
      "pan : pan\n",
      "blenda : blenda\n",
      "puncak : puncak\n",
      "saka : saka\n",
      "nanggulan : nanggulan\n",
      "mung : mung\n",
      "duwur : duwur\n",
      "omah : omah\n",
      "yk : yk\n",
      "cuzz : cuzz\n",
      "besok : besok\n",
      "goleki : golek\n",
      "wah : wah\n",
      "kuat : kuat\n",
      "ngonthel : ngonthel\n",
      "apriyawan : apriyawan\n",
      "feriawan : feriawan\n",
      "putra : putra\n",
      "mantep : mantep\n",
      "ayok : ayok\n",
      "spedaan : spedaan\n",
      "tekan : tekan\n",
      "keneh : keneh\n",
      "waaah : waaah\n",
      "dateng : dateng\n",
      "nit : nit\n",
      "cobain : cobain\n",
      "kuy : kuy\n",
      "adisusanta : adisusanta\n",
      "ikiii : ikiii\n",
      "oii : oii\n",
      "banget : banget\n",
      "viewnya : viewnya\n",
      "mari : mari\n",
      "berpetualang : tualang\n",
      "seru : seru\n",
      "merapi : rapi\n",
      "semoga : moga\n",
      "sehat : sehat\n",
      "abdulk : abdulk\n",
      "azkasetiawan : azkasetiawan\n",
      "kesini : kesini\n",
      "anyar : anyar\n",
      "mneh : mneh\n",
      "sblh : sblh\n",
      "ndi : ndi\n",
      "{'bagus': 'bagus', 'sihh': 'sihh', 'keren': 'keren', 'pemandangan': 'pandang', 'ayoooo': 'ayoooo', 'gasss': 'gasss', 'monggo': 'monggo', 'kemarin': 'kemarin', 'kii': 'kii', 'lhoo': 'lhoo', 'resa': 'resa', 'nama': 'nama', 'tempat': 'tempat', 'ehem': 'ehem', 'zain': 'zain', 'adminnya': 'adminnya', 'kesana': 'kesana', 'belom': 'bom', 'merapat': 'rapat', 'makasih': 'makasih', 'info': 'info', 'nongkrong': 'nongkrong', 'tanjung': 'tanjung', 'harjo': 'harjo', 'syahdu': 'syahdu', 'poll': 'poll', 'donggg': 'donggg', 'gaa': 'gaa', 'pan': 'pan', 'blenda': 'blenda', 'puncak': 'puncak', 'saka': 'saka', 'nanggulan': 'nanggulan', 'mung': 'mung', 'duwur': 'duwur', 'omah': 'omah', 'yk': 'yk', 'cuzz': 'cuzz', 'besok': 'besok', 'goleki': 'golek', 'wah': 'wah', 'kuat': 'kuat', 'ngonthel': 'ngonthel', 'apriyawan': 'apriyawan', 'feriawan': 'feriawan', 'putra': 'putra', 'mantep': 'mantep', 'ayok': 'ayok', 'spedaan': 'spedaan', 'tekan': 'tekan', 'keneh': 'keneh', 'waaah': 'waaah', 'dateng': 'dateng', 'nit': 'nit', 'cobain': 'cobain', 'kuy': 'kuy', 'adisusanta': 'adisusanta', 'ikiii': 'ikiii', 'oii': 'oii', 'banget': 'banget', 'viewnya': 'viewnya', 'mari': 'mari', 'berpetualang': 'tualang', 'seru': 'seru', 'merapi': 'rapi', 'semoga': 'moga', 'sehat': 'sehat', 'abdulk': 'abdulk', 'azkasetiawan': 'azkasetiawan', 'kesini': 'kesini', 'anyar': 'anyar', 'mneh': 'mneh', 'sblh': 'sblh', 'ndi': 'ndi'}\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d59459e75a6412c95951623afa4f29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         [bagus, sihh, keren, pandang]\n",
      "1                                       [ayoooo, gasss]\n",
      "2                                     [monggo, kemarin]\n",
      "3                                           [kii, lhoo]\n",
      "4                                                [resa]\n",
      "5                                        [nama, tempat]\n",
      "6                                                [ehem]\n",
      "7                                                [zain]\n",
      "8                               [adminnya, kesana, bom]\n",
      "9                                                    []\n",
      "10    [rapat, makasih, info, nongkrong, tanjung, harjo]\n",
      "11                                                   []\n",
      "12                                       [syahdu, poll]\n",
      "13                                             [donggg]\n",
      "14                                           [gaa, pan]\n",
      "15                                             [blenda]\n",
      "16                                                   []\n",
      "17            [puncak, saka, tanjung, harjo, nanggulan]\n",
      "18                                  [mung, duwur, omah]\n",
      "19                                       [puncak, saka]\n",
      "20                                                 [yk]\n",
      "21                                                   []\n",
      "22                                                   []\n",
      "23                                 [cuzz, besok, golek]\n",
      "24                                                [wah]\n",
      "25                [kuat, ngonthel, apriyawan, feriawan]\n",
      "26                                      [putra, mantep]\n",
      "27                        [ayok, spedaan, tekan, keneh]\n",
      "28                                                   []\n",
      "29                                      [waaah, dateng]\n",
      "30                                                [nit]\n",
      "31                            [cobain, kuy, adisusanta]\n",
      "32                                                   []\n",
      "33                                         [ikiii, oii]\n",
      "34    [keren, banget, viewnya, mari, tualang, seru, ...\n",
      "35                                             [abdulk]\n",
      "36                          [azkasetiawan, kesini, kuy]\n",
      "37                             [anyar, mneh, sblh, ndi]\n",
      "38                                                   []\n",
      "Name: comment_tokens_stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in LOAD_DATA['comment_normalized']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "LOAD_DATA['comment_tokens_stemmed'] = LOAD_DATA['comment_normalized'].swifter.apply(get_stemmed_term)\n",
    "print(LOAD_DATA['comment_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_DATA.to_csv(\"Text_Preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_DATA.to_excel(\"Text_Preprocessing.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tables'.  Use pip or conda to install tables.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4708\\2752213706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLOAD_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Text_Preprocessing.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\qw\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_hdf\u001b[1;34m(self, path_or_buf, key, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[0;32m   2447\u001b[0m             \u001b[0mdata_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2449\u001b[1;33m             \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2450\u001b[0m         )\n\u001b[0;32m   2451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\qw\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mto_hdf\u001b[1;34m(path_or_buf, key, value, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         with HDFStore(\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplib\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         ) as store:\n\u001b[0;32m    270\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\qw\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format is not a defined argument for HDFStore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_complibs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\qw\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tables'.  Use pip or conda to install tables."
     ]
    }
   ],
   "source": [
    "LOAD_DATA.to_hdf(\"Text_Preprocessing.h5\", 'comment', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
